{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Packt_Neural_Network_Structure.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BTuotdozuJU"
      },
      "source": [
        "# importing Required libraries\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm import tqdm\n",
        "from torchsummary import summary"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDQ4CzYk0Qcj"
      },
      "source": [
        "# Structure-1 (Same as given Notebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zgxo3oFz76G"
      },
      "source": [
        "class Net(nn.Module): # image siE in mnist is 1x28x28\n",
        "                                           \n",
        "\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1) \n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3)\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) # 1 convolutional block\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJJXxP3L0eXk",
        "outputId": "76d8c8eb-0914-4271-ed78-af770f9d36f5"
      },
      "source": [
        "# Checking that cuda is available or not\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4tpgXZ870j96",
        "outputId": "4f90dd2e-25e4-4e02-92a6-84c993a38876"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 7, 7]               0\n",
            "            Conv2d-7            [-1, 512, 5, 5]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 3, 3]       4,719,616\n",
            "            Conv2d-9             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 6,379,786\n",
            "Trainable params: 6,379,786\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.51\n",
            "Params size (MB): 24.34\n",
            "Estimated Total Size (MB): 25.85\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh4eXrOl0pwY",
        "outputId": "58e854ec-79a6-451a-db04-cc9b8bf12f17"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
            "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHmOVYPn0tiC"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3KI1yIy0v0z",
        "outputId": "51cb76ac-62ed-4455-fb9e-48c52b10a3b9"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=1.99245023727417 batch_id=468: 100%|██████████| 469/469 [00:39<00:00, 11.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.9687, Accuracy: 2790/10000 (28%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-U5pPF70xji"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtKkDGHV2qUh"
      },
      "source": [
        "# Structure-2 (Except structure all are same)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLQLefj420a7"
      },
      "source": [
        "class Net(nn.Module): # image siE in mnist is 1x28x28\n",
        "                                           \n",
        "## Without max pooling, use kernels with different shapes and randomly increase as well as decreasing the filter size.\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)    #  32x28x28.\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)   #  64x28x28.\n",
        "        #self.pool1 = nn.MaxPool2d(2, 2)               #  Cancelling the maxpool part.\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)  #  128x28x28.\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1) #  256x28x28.\n",
        "        #self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3)            #  512x26x26\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 5)           #  1024x22x22\n",
        "        self.conv7 = nn.Conv2d(1024, 2048, 7)          #  2048x16x16\n",
        "        self.conv8 = nn.Conv2d(2048, 512, 3)           #  512x14x14\n",
        "        self.conv9 = nn.Conv2d(512, 1024, 7)           #  1024x8x8\n",
        "        self.conv10 = nn.Conv2d(1024, 2048, 5)         #  2048x4x4\n",
        "        self.conv11 = nn.Conv2d(2048, 1024, 3)         #  1024x2x2\n",
        "        self.conv12 = nn.Conv2d(1024, 10, 2)           #  10x1x1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv2(F.relu(self.conv1(x))))   # 1 convolutional block, self.pool1(\n",
        "        x = F.relu(self.conv4(F.relu(self.conv3(x))))   #self.pool2(\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = F.relu(self.conv8(F.relu(self.conv7(x))))\n",
        "        x = F.relu(self.conv10(F.relu(self.conv9(x))))\n",
        "        x = F.relu(self.conv12(F.relu(self.conv11(x))))\n",
        "        \n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Bac8-O3cBW",
        "outputId": "d0feece0-f6e8-464d-a885-a3f1cbc73eda"
      },
      "source": [
        "# Checking that cuda is available or not\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7VhreJS3b9y",
        "outputId": "ede5a327-3749-443c-f262-9b5bbb1a7ae9"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "            Conv2d-3          [-1, 128, 28, 28]          73,856\n",
            "            Conv2d-4          [-1, 256, 28, 28]         295,168\n",
            "            Conv2d-5          [-1, 512, 26, 26]       1,180,160\n",
            "            Conv2d-6         [-1, 1024, 22, 22]      13,108,224\n",
            "            Conv2d-7         [-1, 2048, 16, 16]     102,762,496\n",
            "            Conv2d-8          [-1, 512, 14, 14]       9,437,696\n",
            "            Conv2d-9           [-1, 1024, 8, 8]      25,691,136\n",
            "           Conv2d-10           [-1, 2048, 4, 4]      52,430,848\n",
            "           Conv2d-11           [-1, 1024, 2, 2]      18,875,392\n",
            "           Conv2d-12             [-1, 10, 1, 1]          40,970\n",
            "================================================================\n",
            "Total params: 223,914,762\n",
            "Trainable params: 223,914,762\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 14.84\n",
            "Params size (MB): 854.17\n",
            "Estimated Total Size (MB): 869.01\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXnS4L6e3b2O"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9bBa8FD3byw"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQocos563npg",
        "outputId": "9d708655-44f0-4b73-a62a-ee36a8076ecd"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   ## It is taking more and moere time to train. I think because of the\n",
        "                                                                   ## filters dimension as i chnaged them randomly and lot of number of channels such as 2048, 1024.\n",
        "                                                                   ## I decrease them and also increase them in a random manner.\n",
        "                                                                   ## The next reason may be i did not use the maxpooling and use the kernels randomly \n",
        "                                                                   ## Such as 3x3, 5x5, 7x7. As kernels are high in dimension with high number of filters,\n",
        "                                                                   ## this will be the reason for taking so much time to train.\n",
        "\n",
        "''' it is taking --> \"2h 1m 7s\" to train and give the prediction. The accuracy is also\n",
        "decrease.'''\n",
        "\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=2.3036651611328125 batch_id=468: 100%|██████████| 469/469 [1:54:34<00:00, 14.66s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 2.2995, Accuracy: 1135/10000 (11%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blWzY4VeCEUd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVhkJ7D5CEuO"
      },
      "source": [
        "# Structure-3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAI6RWBN3nl6"
      },
      "source": [
        "class Net(nn.Module): # image siE in mnist is 1x28x28\n",
        "                                           \n",
        "# More maxpooling and more padding.\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)               ## 32x28x28\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)              ## 64x28x28\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                           ## 64x14x14\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)             ## 128x14x14\n",
        "        self.conv4 = nn.Conv2d(128, 256, 3, padding=1)            ## 256x14x14\n",
        "        self.pool2 = nn.MaxPool2d(4, 2)                           ## 256x6x6\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)            ## 512x6x6\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3, padding = 1)         ## 1024x6x6  \n",
        "        \n",
        "        #self.pool3 = nn.MaxPool2d(2, 2)                           ## 1024x5x5\n",
        "        \n",
        "        self.conv7 = nn.Conv2d(1024, 512, 3, padding = 1)         ## 512x6x6\n",
        "        self.pool3 =  nn.MaxPool2d(2, 1)                          ## 512x5x5\n",
        "        self.conv8 = nn.Conv2d(512, 1024, 3)                      ## 1024x3x3\n",
        "        self.conv9 = nn.Conv2d(1024, 10, 3)                       ## 10x1x1\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) # 1 convolutional block\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = F.relu(self.conv6(F.relu(self.conv5(x))))\n",
        "        x = self.pool3(F.relu(self.conv7(x)))\n",
        "        x = F.relu(self.conv9(F.relu(self.conv8(x))))\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvttrjvvPxXP",
        "outputId": "2b7dd8cf-129d-46a9-e32c-8425b6beb20a"
      },
      "source": [
        "# Checking that cuda is available or not\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S013zwrIPyXg",
        "outputId": "f7b7bc6b-1105-433b-d157-736fd096f3c2"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]             320\n",
            "            Conv2d-2           [-1, 64, 28, 28]          18,496\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]         295,168\n",
            "         MaxPool2d-6            [-1, 256, 6, 6]               0\n",
            "            Conv2d-7            [-1, 512, 6, 6]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 6, 6]       4,719,616\n",
            "            Conv2d-9            [-1, 512, 6, 6]       4,719,104\n",
            "        MaxPool2d-10            [-1, 512, 5, 5]               0\n",
            "           Conv2d-11           [-1, 1024, 3, 3]       4,719,616\n",
            "           Conv2d-12             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 15,818,506\n",
            "Trainable params: 15,818,506\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 2.04\n",
            "Params size (MB): 60.34\n",
            "Estimated Total Size (MB): 62.39\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdX1Fph-PyUM"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CU9Wt01XPySb"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_n_kq2OvPyQQ",
        "outputId": "92e91bb3-67af-4117-c263-a40e5865d69f"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   \n",
        "\n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=0.8178346753120422 batch_id=468: 100%|██████████| 469/469 [01:17<00:00,  6.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 0.7775, Accuracy: 7778/10000 (78%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tw3tKDpFPyOA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhCoJ51iPxT2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVLK2SBkQARr"
      },
      "source": [
        "# Structure - 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J1qZ5uubQC3C"
      },
      "source": [
        "class Net(nn.Module): # image siE in mnist is 1x28x28\n",
        "                                           \n",
        "# More maxpooling, bigger kernel size as well as padding size\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 7, padding=3)         ## 32x28x28\n",
        "        self.conv2 = nn.Conv2d(32, 64, 7, padding=3)        ## 64x28x28\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)                     ## 64x14x14\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)       ## 128x14x14\n",
        "        self.conv4 = nn.Conv2d(128, 256, 7, padding=3)      ## 256x14x14\n",
        "        self.pool2 = nn.MaxPool2d(4, 2)                     ## 256x6x6\n",
        "        self.conv5 = nn.Conv2d(256, 512, 3, padding=1)      ## 512x6x6\n",
        "        self.conv6 = nn.Conv2d(512, 1024, 3, padding=1)     ## 1024x6x6\n",
        "        self.pool3 = nn.MaxPool2d(2, 2)                     ## 1024x3x3\n",
        "        self.conv7 = nn.Conv2d(1024, 10, 3)                 ## 10x1x1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(F.relu(self.conv2(F.relu(self.conv1(x))))) # 1 convolutional block\n",
        "        x = self.pool2(F.relu(self.conv4(F.relu(self.conv3(x)))))\n",
        "        x = self.pool3(F.relu(self.conv6(F.relu(self.conv5(x)))))\n",
        "        x = F.relu(self.conv7(x))\n",
        "        x = x.view(-1, 10)\n",
        "        return F.log_softmax(x)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeZsVHnJYuFv",
        "outputId": "aad45652-2bc8-4728-e230-53fecb0d33de"
      },
      "source": [
        "# Checking that cuda is available or not\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nbheefu0YuCf",
        "outputId": "afd18975-dddd-4d84-eda6-e573cefc6cbf"
      },
      "source": [
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "model = Net().to(device)\n",
        "summary(model, input_size=(1, 28, 28))"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 32, 28, 28]           1,600\n",
            "            Conv2d-2           [-1, 64, 28, 28]         100,416\n",
            "         MaxPool2d-3           [-1, 64, 14, 14]               0\n",
            "            Conv2d-4          [-1, 128, 14, 14]          73,856\n",
            "            Conv2d-5          [-1, 256, 14, 14]       1,605,888\n",
            "         MaxPool2d-6            [-1, 256, 6, 6]               0\n",
            "            Conv2d-7            [-1, 512, 6, 6]       1,180,160\n",
            "            Conv2d-8           [-1, 1024, 6, 6]       4,719,616\n",
            "         MaxPool2d-9           [-1, 1024, 3, 3]               0\n",
            "           Conv2d-10             [-1, 10, 1, 1]          92,170\n",
            "================================================================\n",
            "Total params: 7,773,706\n",
            "Trainable params: 7,773,706\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.00\n",
            "Forward/backward pass size (MB): 1.81\n",
            "Params size (MB): 29.65\n",
            "Estimated Total Size (MB): 31.46\n",
            "----------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5DONl5NYuA-"
      },
      "source": [
        "torch.manual_seed(1)\n",
        "batch_size = 128\n",
        "\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                    transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.1307,), (0.3081,))\n",
        "                    ])),\n",
        "    batch_size=batch_size, shuffle=True, **kwargs)\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aj_dKdQYt-C"
      },
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    pbar = tqdm(train_loader)\n",
        "    for batch_idx, (data, target) in enumerate(pbar):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8hP61gRYt75",
        "outputId": "3d06e79e-cf02-4f57-de96-c6bbeb434bc4"
      },
      "source": [
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)   \n",
        "\n",
        "for epoch in range(1, 2):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/469 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:24: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "loss=1.3390775918960571 batch_id=468: 100%|██████████| 469/469 [01:25<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test set: Average loss: 1.2694, Accuracy: 5661/10000 (57%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDQgmqybYt5x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TdnQ3l5DYtz9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}